{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reimplement simple NN in python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKz2L9vM6ftrS1e/lK7yWq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShengpeiWang/Reimplement_simple_NN/blob/master/Reimplement_simple_NN_in_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQNXFGF0DD2z",
        "colab_type": "text"
      },
      "source": [
        "# Reimplementing a simple neural network is a good to way to learn DL\n",
        "- In my other project applying BERT model to text classification, I struggled with modifying other people's PyTorch code to my purpose. Although I understood generally what the different components did, my struggle impelled me to gain a deeper understanding of PyTorch. And there’s no better way to do that than reimplementing some of PyTorch’s basic functionalities from the ground up. This is also giving me a deeper understanding of deep learning fundamentals.    \n",
        "- This is part of my notes from the fast.ai course (part 2), where I reimplemented some of PyTorch’s functionalities in their barest forms. I often learn by looking at other people’s code, and hopefully this can help other DL beginners to start playing with their models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI-wGmxBCxbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "from IPython.core.debugger import set_trace\n",
        "from fastai import datasets\n",
        "import pickle, gzip, math, torch, matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import tensor\n",
        "\n",
        "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r40Tq2quH9sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# functions to check that out code work here\n",
        "def test(a,b,cmp,cname=None):\n",
        "    if cname is None: cname=cmp.__name__\n",
        "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
        "\n",
        "def near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\n",
        "def test_near(a,b): test(a,b,near)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA77zEUHcfN7",
        "colab_type": "text"
      },
      "source": [
        "## Get the MNIST data\n",
        "Which data doesn't really matter for my purpose here. The MNIST is a simple problem to start with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4efvv4e0bt6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data():\n",
        "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
        "    with gzip.open(path, 'rb') as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
        "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
        "\n",
        "def normalize(x, m, s): return (x-m)/s"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtbPAhBFDiV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "ac7e302f-6a60-4697-9650-4220c6ace550"
      },
      "source": [
        "x_train,y_train,x_valid,y_valid = get_data()\n",
        "x_train, x_train.shape, y_train, y_train.shape, y_train.min(), y_train.max()\n",
        "# Look at the shapes of the data, making sure they are sensible"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " torch.Size([50000, 784]),\n",
              " tensor([5, 0, 4,  ..., 8, 4, 8]),\n",
              " torch.Size([50000]),\n",
              " tensor(0),\n",
              " tensor(9))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80X0TRsuD3UG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5beee03f-1c9a-4140-b46b-fa1123f61bb1"
      },
      "source": [
        "# look at the first image:\n",
        "mpl.rcParams['image.cmap'] = 'gray'\n",
        "img = x_train[0]\n",
        "plt.imshow(img.view((28,28)));"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN9klEQVR4nO3df4xV9ZnH8c+zWP6QojBrOhKKSyEGg8ZON4gbl6w1hvojGhw1TSexoZE4/YNJaLIhNewf1WwwZBU2SzTNTKMWNl1qEzUgaQouoOzGhDgiKo5LdQ2mTEaowZEf/mCHefaPezBTnfu9w7nn3nOZ5/1Kbu6957nnnicnfDi/7pmvubsATH5/VXYDAJqDsANBEHYgCMIOBEHYgSAuaubCzIxT/0CDubuNN72uLbuZ3Wpmh8zsPTN7sJ7vAtBYlvc6u5lNkfRHSUslHZH0qqQudx9IzMOWHWiwRmzZF0t6z93fd/czkn4raVkd3weggeoJ+2xJfxrz/kg27S+YWbeZ9ZtZfx3LAlCnhp+gc/c+SX0Su/FAmerZsg9KmjPm/bezaQBaUD1hf1XSlWb2HTObKulHkrYV0xaAouXejXf3ETPrkbRD0hRJT7n724V1BqBQuS+95VoYx+xAwzXkRzUALhyEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBJF7yGZcGKZMmZKsX3rppQ1dfk9PT9XaxRdfnJx3wYIFyfrKlSuT9ccee6xqraurKznv559/nqyvW7cuWX/44YeT9TLUFXYzOyzppKSzkkbcfVERTQEoXhFb9pvc/aMCvgdAA3HMDgRRb9hd0k4ze83Musf7gJl1m1m/mfXXuSwAdah3N36Juw+a2bckvWhm/+Pue8d+wN37JPVJkpl5ncsDkFNdW3Z3H8yej0l6XtLiIpoCULzcYTezaWY2/dxrST+QdLCoxgAUq57d+HZJz5vZue/5D3f/QyFdTTJXXHFFsj516tRk/YYbbkjWlyxZUrU2Y8aM5Lz33HNPsl6mI0eOJOsbN25M1js7O6vWTp48mZz3jTfeSNZffvnlZL0V5Q67u78v6bsF9gKggbj0BgRB2IEgCDsQBGEHgiDsQBDm3rwftU3WX9B1dHQk67t3707WG32baasaHR1N1u+///5k/dSpU7mXPTQ0lKx//PHHyfqhQ4dyL7vR3N3Gm86WHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeC4Dp7Adra2pL1ffv2Jevz5s0rsp1C1ep9eHg4Wb/pppuq1s6cOZOcN+rvD+rFdXYgOMIOBEHYgSAIOxAEYQeCIOxAEIQdCIIhmwtw/PjxZH316tXJ+h133JGsv/7668l6rT+pnHLgwIFkfenSpcn66dOnk/Wrr766am3VqlXJeVEstuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EAT3s7eASy65JFmvNbxwb29v1dqKFSuS8953333J+pYtW5J1tJ7c97Ob2VNmdszMDo6Z1mZmL5rZu9nzzCKbBVC8iezG/1rSrV+Z9qCkXe5+paRd2XsALaxm2N19r6Sv/h50maRN2etNku4quC8ABcv72/h2dz83WNaHktqrfdDMuiV151wOgILUfSOMu3vqxJu790nqkzhBB5Qp76W3o2Y2S5Ky52PFtQSgEfKGfZuk5dnr5ZK2FtMOgEapuRtvZlskfV/SZWZ2RNIvJK2T9DszWyHpA0k/bGSTk92JEyfqmv+TTz7JPe8DDzyQrD/zzDPJeq0x1tE6aobd3buqlG4uuBcADcTPZYEgCDsQBGEHgiDsQBCEHQiCW1wngWnTplWtvfDCC8l5b7zxxmT9tttuS9Z37tyZrKP5GLIZCI6wA0EQdiAIwg4EQdiBIAg7EARhB4LgOvskN3/+/GR9//79yfrw8HCyvmfPnmS9v7+/au2JJ55IztvMf5uTCdfZgeAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIrrMH19nZmaw//fTTyfr06dNzL3vNmjXJ+ubNm5P1oaGhZD0qrrMDwRF2IAjCDgRB2IEgCDsQBGEHgiDsQBBcZ0fSNddck6xv2LAhWb/55vyD/fb29ibra9euTdYHBwdzL/tClvs6u5k9ZWbHzOzgmGkPmdmgmR3IHrcX2SyA4k1kN/7Xkm4dZ/q/untH9vh9sW0BKFrNsLv7XknHm9ALgAaq5wRdj5m9me3mz6z2ITPrNrN+M6v+x8gANFzesP9S0nxJHZKGJK2v9kF373P3Re6+KOeyABQgV9jd/ai7n3X3UUm/krS42LYAFC1X2M1s1pi3nZIOVvssgNZQ8zq7mW2R9H1Jl0k6KukX2fsOSS7psKSfunvNm4u5zj75zJgxI1m/8847q9Zq3StvNu7l4i/t3r07WV+6dGmyPllVu85+0QRm7Bpn8pN1dwSgqfi5LBAEYQeCIOxAEIQdCIKwA0FwiytK88UXXyTrF12Uvlg0MjKSrN9yyy1Vay+99FJy3gsZf0oaCI6wA0EQdiAIwg4EQdiBIAg7EARhB4KoedcbYrv22muT9XvvvTdZv+6666rWal1Hr2VgYCBZ37t3b13fP9mwZQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBILjOPsktWLAgWe/p6UnW77777mT98ssvP++eJurs2bPJ+tBQ+q+Xj46OFtnOBY8tOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwXX2C0Cta9ldXeMNtFtR6zr63Llz87RUiP7+/mR97dq1yfq2bduKbGfSq7llN7M5ZrbHzAbM7G0zW5VNbzOzF83s3ex5ZuPbBZDXRHbjRyT9o7svlPR3klaa2UJJD0ra5e5XStqVvQfQomqG3d2H3H1/9vqkpHckzZa0TNKm7GObJN3VqCYB1O+8jtnNbK6k70naJ6nd3c/9OPlDSe1V5umW1J2/RQBFmPDZeDP7pqRnJf3M3U+MrXlldMhxB2109z53X+Tui+rqFEBdJhR2M/uGKkH/jbs/l00+amazsvosScca0yKAItTcjTczk/SkpHfcfcOY0jZJyyWty563NqTDSaC9fdwjnC8tXLgwWX/88ceT9auuuuq8eyrKvn37kvVHH320am3r1vQ/GW5RLdZEjtn/XtKPJb1lZgeyaWtUCfnvzGyFpA8k/bAxLQIoQs2wu/t/Sxp3cHdJNxfbDoBG4eeyQBCEHQiCsANBEHYgCMIOBMEtrhPU1tZWtdbb25uct6OjI1mfN29erp6K8MorryTr69evT9Z37NiRrH/22Wfn3RMagy07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgQR5jr79ddfn6yvXr06WV+8eHHV2uzZs3P1VJRPP/20am3jxo3JeR955JFk/fTp07l6Quthyw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQYS5zt7Z2VlXvR4DAwPJ+vbt25P1kZGRZD11z/nw8HByXsTBlh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgjB3T3/AbI6kzZLaJbmkPnf/NzN7SNIDkv6cfXSNu/++xnelFwagbu4+7qjLEwn7LEmz3H2/mU2X9Jqku1QZj/2Uuz820SYIO9B41cI+kfHZhyQNZa9Pmtk7ksr90ywAztt5HbOb2VxJ35O0L5vUY2ZvmtlTZjazyjzdZtZvZv11dQqgLjV347/8oNk3Jb0saa27P2dm7ZI+UuU4/p9V2dW/v8Z3sBsPNFjuY3ZJMrNvSNouaYe7bxinPlfSdne/psb3EHagwaqFveZuvJmZpCclvTM26NmJu3M6JR2st0kAjTORs/FLJP2XpLckjWaT10jqktShym78YUk/zU7mpb6LLTvQYHXtxheFsAONl3s3HsDkQNiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQii2UM2fyTpgzHvL8umtaJW7a1V+5LoLa8ie/ubaoWm3s/+tYWb9bv7otIaSGjV3lq1L4ne8mpWb+zGA0EQdiCIssPeV/LyU1q1t1btS6K3vJrSW6nH7ACap+wtO4AmIexAEKWE3cxuNbNDZvaemT1YRg/VmNlhM3vLzA6UPT5dNobeMTM7OGZam5m9aGbvZs/jjrFXUm8Pmdlgtu4OmNntJfU2x8z2mNmAmb1tZquy6aWuu0RfTVlvTT9mN7Mpkv4oaamkI5JeldTl7gNNbaQKMzssaZG7l/4DDDP7B0mnJG0+N7SWmf2LpOPuvi77j3Kmu/+8RXp7SOc5jHeDeqs2zPhPVOK6K3L48zzK2LIvlvSeu7/v7mck/VbSshL6aHnuvlfS8a9MXiZpU/Z6kyr/WJquSm8twd2H3H1/9vqkpHPDjJe67hJ9NUUZYZ8t6U9j3h9Ra4337pJ2mtlrZtZddjPjaB8zzNaHktrLbGYcNYfxbqavDDPeMusuz/Dn9eIE3dctcfe/lXSbpJXZ7mpL8soxWCtdO/2lpPmqjAE4JGl9mc1kw4w/K+ln7n5ibK3MdTdOX01Zb2WEfVDSnDHvv51NawnuPpg9H5P0vCqHHa3k6LkRdLPnYyX38yV3P+ruZ919VNKvVOK6y4YZf1bSb9z9uWxy6etuvL6atd7KCPurkq40s++Y2VRJP5K0rYQ+vsbMpmUnTmRm0yT9QK03FPU2Scuz18slbS2xl7/QKsN4VxtmXCWvu9KHP3f3pj8k3a7KGfn/lfRPZfRQpa95kt7IHm+X3ZukLars1v2fKuc2Vkj6a0m7JL0r6T8ltbVQb/+uytDeb6oSrFkl9bZElV30NyUdyB63l73uEn01Zb3xc1kgCE7QAUEQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ/w8ie3GmjcGk5QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwQDbZIWkmR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_mean,train_std = x_train.mean(),x_train.std()\n",
        "x_train = normalize(x_train, train_mean, train_std)\n",
        "# NB: Use training, not validation mean for validation set\n",
        "x_valid = normalize(x_valid, train_mean, train_std)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKfoy9WkdYqW",
        "colab_type": "text"
      },
      "source": [
        "## Pythonic implementation of matrix multiplication\n",
        "Matrix multiplication is fundamental to neural networks, it's worth knowing a little how to implement it in code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwqDy2HfIwfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def matmul(a, b):\n",
        "  ar, ac = a.shape\n",
        "  br, bc = b.shape\n",
        "  assert ac == br\n",
        "  c = torch.zeros(ar, bc)\n",
        "  for i in range(ar):\n",
        "    c[i,:] =  (a[i][:,None] * b).sum(dim = 0)\n",
        "  return c"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kRq1UlAP_f1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fc01fbe3-2c56-4289-822b-88eab733661b"
      },
      "source": [
        "a = tensor([10,20,30,40,50,60]).view(2,3)\n",
        "b = tensor([1,2,3,4,5,6]).view(3,2)\n",
        "b"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4],\n",
              "        [5, 6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch_ylrfqSDLl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b754ae21-6448-442f-e8fc-e46db8d72c3a"
      },
      "source": [
        "matmul(a,b)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[220., 280.],\n",
              "        [490., 640.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG6H_2aPQeQd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "39b221e2-aa49-40dc-fe4f-32ce7e6491df"
      },
      "source": [
        "# compare with pytorch's operation\n",
        "a.matmul(b)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[220, 280],\n",
              "        [490, 640]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvpHvp6bX7LS",
        "colab_type": "text"
      },
      "source": [
        "## Initialize a simple model with a single hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTZdbE4ck9zH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "24f4c923-9b78-4b85-faad-df054bd809e3"
      },
      "source": [
        "n,m = x_train.shape # input shape\n",
        "c = y_train.max()+1 # output shape\n",
        "nh = 50 # num hidden cells\n",
        "n,m,c"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 784, tensor(10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nvy-PXElJ0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize the weights and biases\n",
        "# simplified kaiming init / he init\n",
        "w1 = torch.randn(m,nh)*math.sqrt(2/(1+m)) # note that I added a 1 to m (originally math.sqrt(2/(m))) to better replicate the ResNet paper's implementation \n",
        "b1 = torch.zeros(nh)\n",
        "w2 = torch.randn(nh,1)/math.sqrt(nh) # this is different than w1 because there is not a relu layer after like the first layer\n",
        "b2 = torch.zeros(1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COzGJeKjuUyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "44bf1677-61a7-4366-b83b-eebf560be591"
      },
      "source": [
        "# here is how to do the initializing using pytorch:\n",
        "from torch.nn import init\n",
        "w1 = torch.zeros(m,nh)\n",
        "init.kaiming_normal_(w1, mode='fan_out') # use the pytorch version of the kaiming init"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0091, -0.0519, -0.0576,  ...,  0.1136,  0.0081, -0.0036],\n",
              "        [ 0.1169,  0.0147,  0.0049,  ..., -0.0246,  0.0364,  0.0220],\n",
              "        [-0.0542,  0.0476, -0.0105,  ...,  0.0496,  0.0189, -0.0666],\n",
              "        ...,\n",
              "        [ 0.0815, -0.0367,  0.0296,  ...,  0.0790, -0.0221, -0.0071],\n",
              "        [ 0.0100,  0.0424, -0.0246,  ..., -0.1029, -0.0214,  0.0361],\n",
              "        [ 0.0840, -0.0197, -0.0790,  ...,  0.0654, -0.0302,  0.0347]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQaDkrawkSyf",
        "colab_type": "text"
      },
      "source": [
        "# Implement forward and backward passes of a linear layer, Rely, and MSE\n",
        "For each component, the forward pass is calculated when the class is called, and the backward pass is calculated when the backward function is called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NONUPOXbVHEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Relu():\n",
        "  def __call__(self, inp):\n",
        "    self.inp = inp\n",
        "    self.out = self.inp.clamp_min(0.) - 0.5 # the -0.5 is to facilitate training by making the outputs with mean close to zero and standard deviation close to 1\n",
        "    return self.out\n",
        "  \n",
        "  def backward(self):\n",
        "    self.inp.g = (self.inp > 0).float() * self.out.g"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn6dDAduY4ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lin():\n",
        "  def __init__(self, w, b):\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "\n",
        "  def __call__(self, inp):\n",
        "    self.inp = inp\n",
        "    self.out = self.inp @ self.w + self.b\n",
        "    return self.out\n",
        "\n",
        "  def backward(self):\n",
        "    self.inp.g = self.out.g @ self.w.t()\n",
        "    self.w.g = self.inp.t() @ self.out.g\n",
        "    self.b.g = self.out.g.sum(0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIh4AlinbdV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mse():\n",
        "  def __call__(self, inp, targ):\n",
        "    self.inp = inp\n",
        "    self.targ = targ\n",
        "    self.out = (self.inp.squeeze(1) - self.targ).pow(2).mean()\n",
        "    return self.out\n",
        "\n",
        "  def backward(self):\n",
        "    self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.inp.shape[0]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQzPjO_acOA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model():\n",
        "  def __init__(self, w1, b1, w2, b2):\n",
        "    self.layers = [Lin(w1, b1), Relu(), Lin(w2, b2)]\n",
        "    self.loss = Mse()\n",
        "  \n",
        "  def __call__(self, x, targ):\n",
        "    for l in self.layers: x = l(x)\n",
        "    return self.loss(x, targ)\n",
        "\n",
        "  def backward(self):\n",
        "    self.loss.backward()\n",
        "    for l in reversed(self.layers): l.backward()\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6B9UJy5dejB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1.g,b1.g,w2.g,b2.g = [None]*4\n",
        "model = Model(w1, b1, w2, b2)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-nSsEy_dgoY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a0019d79-ce85-4bd3-844c-8cfa0cd4eb3c"
      },
      "source": [
        "%time loss = model(x_train, y_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 133 ms, sys: 0 ns, total: 133 ms\n",
            "Wall time: 145 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_LQfPlOeio2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "81e474f5-b397-4836-efe1-0d88dc5b6629"
      },
      "source": [
        "loss"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(33.6131)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwBaGB8qd_oz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f8f46a87-8ccf-4ad9-9574-aca45c677834"
      },
      "source": [
        "%time model.backward()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 225 ms, sys: 1.88 ms, total: 227 ms\n",
            "Wall time: 228 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63P9K-CEfiqU",
        "colab_type": "text"
      },
      "source": [
        "## Refactor the repetetive code using the Module class\n",
        "- The module class handles assigning arguments, and calling the forward and backward passes. \n",
        "- This makes the code both faster and easier to read (when things get more complicated)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qDGpoQYfbis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Module():\n",
        "  def __call__(self, *args):\n",
        "    self.args = args\n",
        "    self.out = self.forward(*args)\n",
        "    return self.out\n",
        "  \n",
        "  def forward(self): raise Exception('not implemented')\n",
        "  def backward(self): self.bwd(self.out, *self.args)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_TNRRZHtKVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example class to test how the Module class work\n",
        "# I was rather cofused about how the Module class works, so I wrote this to help me dissect all the name spaces\n",
        "class print_args(Module):\n",
        "  def __init__(self, a, b):\n",
        "    self.a = a\n",
        "    self.b = b\n",
        "  \n",
        "  def forward(self, inp1, inp2):\n",
        "    return [self.a, self.b, self.b, inp1, inp2]\n",
        "  \n",
        "  def bwd(self, out, inp1, inp2): \n",
        "  # note that 'out' has to be the first argument after self, because how bwd is called in the module class\n",
        "  # also note that all of the arguments given to the forward method need to be listed here, otherwise a TypeError is raised \n",
        "    print(\"backward\", out, inp1, inp2, self.a, self.b)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj_aj3n9tsxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = print_args(\"first\", \"second\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX2y1lxDuOs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "06dd68fe-e3e7-47e6-b517-7eb0703f5aa9"
      },
      "source": [
        "a(\"input1\", \"input2\") "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['first', 'second', 'second', 'input1', 'input2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc0LlAELvXtG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "432f9edf-6b97-4543-90b4-ebf1a3d860b6"
      },
      "source": [
        "a.backward()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "backward ['first', 'second', 'second', 'input1', 'input2'] input1 input2 first second\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-2qUYNgo0tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Relu(Module):\n",
        "  def forward(self, inp):   return inp.clamp_min(0.) - 0.5\n",
        "  def bwd(self, out, inp):  inp.g = (inp > 0).float() * out.g"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZu_EsnSsfoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lin(Module):\n",
        "  def __init__(self, w, b):\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "\n",
        "  def forward(self, inp): return inp @ self.w + self.b\n",
        "  \n",
        "  def bwd(self, out, inp):\n",
        "    inp.g = out.g @ self.w.t()\n",
        "    self.w.g = inp.t() @ out.g\n",
        "    self.b.g = out.g.sum(0)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ghoaWswKIQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mse(Module):\n",
        "  def forward(self, inp, targ):\n",
        "    return (inp.squeeze(1) - targ).pow(2).mean()\n",
        "\n",
        "  def bwd(self, out, inp, targ):\n",
        "    inp.g = 2. * (inp.squeeze() -  targ).unsqueeze(-1) / inp.shape[0]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pMHAS0hLqwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model():\n",
        "  def __init__(self):\n",
        "    self.layer = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
        "    self.loss = Mse()\n",
        "\n",
        "  def __call__(self, x, targ):\n",
        "    for l in self.layer: x = l(x)\n",
        "    return self.loss(x, targ)\n",
        "\n",
        "  def backward(self):\n",
        "    self.loss.backward()\n",
        "    for l in reversed(self.layer): l.backward()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP6C9NV7MjGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1.g,b1.g,w2.g,b2.g = [None]*4\n",
        "model = Model()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54UYK_4pMm4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5b4d180d-f6ed-49b5-e3da-cf8dc4635b02"
      },
      "source": [
        "%time loss = model(x_train, y_train)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 134 ms, sys: 1.56 ms, total: 135 ms\n",
            "Wall time: 140 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acN2kAG0nKrf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f3adf2a7-c9ad-4163-efc3-980a0a1c0dff"
      },
      "source": [
        "loss"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(33.6131)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5a80rLaUv7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "881d78ad-c3ed-4bb0-8edf-6a8099c76157"
      },
      "source": [
        "%time model.backward()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 221 ms, sys: 82.2 ms, total: 303 ms\n",
            "Wall time: 305 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48riSVKKVfb6",
        "colab_type": "text"
      },
      "source": [
        "### compare our code with pytorch's implementations: nn.Module and nn.*Linear*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDNPNj3MVPtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CKkv4SwVyKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n",
        "        self.loss = mse\n",
        "        \n",
        "    def __call__(self, x, targ):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return self.loss(x, targ)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXbgwNFCWBpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(m, nh, 1)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax53MC0_WI4B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aacd9e15-1df6-44f0-e8eb-a3b330e7cd24"
      },
      "source": [
        "%time loss = model(x_train, y_train)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 111 ms, sys: 946 µs, total: 112 ms\n",
            "Wall time: 114 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvBg090cnIn7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "38581ec2-d7d4-420a-bde8-f0661b16803e"
      },
      "source": [
        "loss\n",
        "# Note that because the relu is implemented differently in our code than Pytorch's, the loss is different here"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(26.4548, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK4D5n6MWLvy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b4532147-79f9-4b17-8a7b-94514ce8b95b"
      },
      "source": [
        "%time loss.backward()\n",
        "# PyTorch's implementation of backward is twice as fast"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 88.1 ms, sys: 3.65 ms, total: 91.7 ms\n",
            "Wall time: 97.3 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}